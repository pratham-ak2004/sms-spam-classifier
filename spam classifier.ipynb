{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./src/spam.csv\",encoding=\"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=[\"Unnamed: 2\",\"Unnamed: 3\",\"Unnamed: 4\"],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = [\"target\",\"message\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "encoder = LabelEncoder()\n",
    "df[\"target\"] = encoder.fit_transform(df[\"target\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates(keep = \"first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"target\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.pie(df[\"target\"].value_counts(),labels=[\"ham\",\"spam\"],autopct=\"%.3f\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## !pip install nltk\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## extracting number of characters\n",
    "df[\"characters\"] = df[\"message\"].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## extracting number of words\n",
    "df[\"words\"] = df[\"message\"].apply(lambda x : len(nltk.word_tokenize(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"sentences\"] = df[\"message\"].apply(lambda x : len(nltk.sent_tokenize(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ham meassages\n",
    "df[df[\"target\"] == 0].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spam messages\n",
    "df[df[\"target\"] == 1].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "sns.histplot(df[df[\"target\"] == 0][\"characters\"],color=\"blue\")\n",
    "sns.histplot(df[df[\"target\"] == 1][\"characters\"],color=\"red\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "sns.histplot(df[df[\"target\"] == 0][\"words\"],color=\"blue\")\n",
    "sns.histplot(df[df[\"target\"] == 1][\"words\"],color=\"red\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "sns.histplot(df[df[\"target\"] == 0][\"sentences\"],color=\"blue\")\n",
    "sns.histplot(df[df[\"target\"] == 1][\"sentences\"],color=\"red\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "sns.pairplot(df,hue=\"target\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import string\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# function to apply text preprocessing\n",
    "def text_transform(text):\n",
    "    # convert to lower case\n",
    "    text = text.lower()\n",
    "    # tokenize the text to individual words\n",
    "    text = nltk.word_tokenize(text)\n",
    "    # remove special characters and convert\n",
    "    lst = []\n",
    "    for word in text:\n",
    "        # stopwords : which helps in formation of sentences and has no special meaning\n",
    "        if word.isalnum() and word not in stopwords.words(\"english\") and word not in string.punctuation:\n",
    "            lst.append(stemmer.stem(word))\n",
    "\n",
    "    return \" \".join(lst)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"transformed text\"] = df[\"message\"].apply(lambda x : text_transform(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# representation using wordcloud\n",
    "from wordcloud import WordCloud\n",
    "wc = WordCloud(height=500,width=500,min_font_size=5,background_color=\"white\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wordcloud of ham\n",
    "plt.imshow(wc.generate(df[df[\"target\"] == 0][\"transformed text\"].str.cat(sep=\" \"))) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wordcloud of spam\n",
    "plt.imshow(wc.generate(df[df[\"target\"] == 1][\"transformed text\"].str.cat(sep=\" \"))) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_message(lst):\n",
    "    word_list = []\n",
    "    for msg in lst:\n",
    "        for word in msg.split():\n",
    "            word_list.append(word)\n",
    "    return word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_lst = count_message(df[df[\"target\"] == 1][\"transformed text\"].tolist())\n",
    "ham_lst = count_message(df[df[\"target\"] == 0][\"transformed text\"].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total words in ham : \",len(ham_lst))\n",
    "print(\"Total words in spam : \",len(spam_lst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "spam_df = pd.DataFrame(Counter(spam_lst).most_common(10))\n",
    "ham_df = pd.DataFrame(Counter(ham_lst).most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ham_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Text conversion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## https://pages.github.rpi.edu/kuruzj/website_introml_rpi/notebooks/08-intro-nlp/03-scikit-learn-text.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_words = vectorizer.fit_transform(df[\"transformed text\"]).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf-idf encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## https://pages.github.rpi.edu/kuruzj/website_introml_rpi/notebooks/08-intro-nlp/03-scikit-learn-text.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tf_idf = vectorizer.fit_transform(df[\"transformed text\"]).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tf_idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### word2vec encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## https://youtu.be/hQwFeIupNP0\n",
    "## https://youtu.be/hQwFeIupNP0\n",
    "## https://www.hackersrealm.net/post/word2vec-python#:~:text=Word2Vec%20is%20a%20popular%20technique,can%20be%20used%20in%20python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(vector_size = 100 , min_count = 1 , window = 2 , workers= 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"temp\"] = df[\"transformed text\"].apply(lambda x : x.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.build_vocab(df[\"temp\"])\n",
    "model.train(df[\"temp\"], total_examples=model.corpus_count, epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop([\"temp\"],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"./src/word2vec.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Training models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier as DTC\n",
    "from sklearn.neighbors import KNeighborsClassifier as KNN \n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import  GaussianNB , MultinomialNB , BernoulliNB\n",
    "from sklearn.ensemble import ExtraTreesClassifier as ETC , RandomForestClassifier as RF\n",
    "\n",
    "from sklearn.metrics import accuracy_score , precision_score , confusion_matrix\n",
    "from sklearn.model_selection import train_test_split as tt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "knn = Pipeline([('Scalar 1',StandardScaler()) , ('PCA 1' , PCA(n_components=6)) , ('KNN Classifier' , KNN())])\n",
    "dtc = Pipeline([('Scalar 2',StandardScaler()) , ('PCA 2' , PCA(n_components=6)) , ('DTC Classifier' , DTC())])\n",
    "svc = Pipeline([('Scalar 3',StandardScaler()) , ('PCA 3' , PCA(n_components=6)) , ('SVC Classifier' , SVC())])\n",
    "gnb = Pipeline([('Scalar 4',StandardScaler()) , ('PCA 4' , PCA(n_components=6)) , ('Gaussian Classifier' , GaussianNB())])\n",
    "mnb = Pipeline([('Scalar 5',StandardScaler()) , ('PCA 5' , PCA(n_components=6)) , ('Multinomial Classifier' , MultinomialNB())])\n",
    "bnb = Pipeline([('Scalar 6',StandardScaler()) , ('PCA 6' , PCA(n_components=6)) , ('Bernoulli Classifier' , BernoulliNB())])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = Pipeline([('KNN Classifier' , KNN())])\n",
    "dtc = Pipeline([('DTC Classifier' , DTC())])\n",
    "svc = Pipeline([('SVC Classifier' , SVC())])\n",
    "etc = Pipeline([('etc Classifier' , ETC())])\n",
    "rf = Pipeline([('rf Classifier' ,RF())])\n",
    "gnb = Pipeline([('Gaussian Classifier' , GaussianNB())])\n",
    "mnb = Pipeline([('Multinomial Classifier' , MultinomialNB())])\n",
    "bnb = Pipeline([('Bernoulli Classifier' , BernoulliNB())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelines = [knn,dtc,svc,gnb,mnb,bnb,etc,rf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_dict = {\n",
    "    0 : \"KNN\",\n",
    "    1 : \"DTC\", \n",
    "    2 : \"SVC\", \n",
    "    3 : \"GNB\", \n",
    "    4 : \"MNB\", \n",
    "    5 : \"BNB\",\n",
    "    6 : \"ETC\",\n",
    "    7 : \"RF\"\n",
    "    }\n",
    "\n",
    "model_dict = pd.DataFrame.from_dict(pipe_dict,orient=\"index\",columns=[\"models\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training with bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train , x_test , y_train , y_test = tt(bag_of_words,df[\"target\"],test_size = 0.3,random_state=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pipe in pipelines:\n",
    "    try:\n",
    "        pipe.fit(x_train,y_train)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy , precision = [] , []\n",
    "for i , model in enumerate(pipelines):\n",
    "    try:\n",
    "        x_pred = model.predict(x_test)\n",
    "        accuracy.append(accuracy_score(x_pred,y_test))\n",
    "        precision.append(precision_score(x_pred,y_test))\n",
    "        print(f\"{pipe_dict[i]} confusion matrix : \\n\",confusion_matrix(x_pred,y_test))\n",
    "        ## print(f\"{pipe_dict[i]} Test Accuracy : {accuracy_score(x_pred,y_test)}\")\n",
    "    except:\n",
    "        accuracy.append(0)\n",
    "        precision.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict[\"Bag of words Accuracy\"] = pd.DataFrame(accuracy,columns=[\"Bag of words accuracy\"])\n",
    "model_dict[\"Bag of words Precision\"] = pd.DataFrame(precision,columns=[\"Bag of words precision\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training with tf-idf encoded data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train , x_test , y_train , y_test = tt(Tf_idf,df[\"target\"],test_size = 0.3,random_state=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pipe in pipelines:\n",
    "    try:\n",
    "        pipe.fit(x_train,y_train)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy , precision = [] , []\n",
    "for i , model in enumerate(pipelines):\n",
    "    try:\n",
    "        x_pred = model.predict(x_test)\n",
    "        accuracy.append(accuracy_score(x_pred,y_test))\n",
    "        precision.append(precision_score(x_pred,y_test))\n",
    "        print(f\"{pipe_dict[i]} confusion matrix : \\n\",confusion_matrix(x_pred,y_test))\n",
    "        ## print(f\"{pipe_dict[i]} Test Accuracy : {accuracy_score(x_pred,y_test)}\")\n",
    "    except:\n",
    "        accuracy.append(0)\n",
    "        precision.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict[\"Tf-idf Accuracy\"] = pd.DataFrame(accuracy,columns=[\"Tf-idf accuracy\"])\n",
    "model_dict[\"Tf-idf Precision\"] = pd.DataFrame(precision,columns=[\"Tf-idf precision\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict ## bag of words -> mnb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training on word2vec encoded text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = Word2Vec.load(\"./src/word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_text(text):\n",
    "  ## create text vector\n",
    "  text_vector = np.zeros(word2vec.vector_size)\n",
    "  \n",
    "  count = 0\n",
    "  for word in text.split():\n",
    "    if word in word2vec.wv:\n",
    "      text_vector += word2vec.wv[word]\n",
    "      count += 1\n",
    "\n",
    "  if count != 0:\n",
    "        text_vector /= count\n",
    "  return text_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = []\n",
    "for it in df[\"transformed text\"]:\n",
    "    x.append(encode_text(it))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train , x_test , y_train , y_test = tt(x,df[\"target\"],test_size = 0.3,random_state=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pipe in pipelines:\n",
    "    try:\n",
    "        pipe.fit(x_train,y_train)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy , precision = [] , []\n",
    "for i , model in enumerate(pipelines):\n",
    "    try:\n",
    "        x_pred = model.predict(x_test)\n",
    "        accuracy.append(accuracy_score(x_pred,y_test))\n",
    "        precision.append(precision_score(x_pred,y_test))\n",
    "        print(f\"{pipe_dict[i]} confusion matrix : \\n\",confusion_matrix(x_pred,y_test))\n",
    "        ## print(f\"{pipe_dict[i]} Test Accuracy : {accuracy_score(x_pred,y_test)}\")\n",
    "    except:\n",
    "        accuracy.append(0)\n",
    "        precision.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict[\"Word2Vec Accuracy\"] = pd.DataFrame(accuracy,columns=[\"Word2Vec accuracy\"])\n",
    "model_dict[\"Word2Vec Precision\"] = pd.DataFrame(precision,columns=[\"Word2Vec precision\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multinomial naive bayes trained with bag of words give the highest accuracy and percision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import  MultinomialNB\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "classifier = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split as tt\n",
    "x = vectorizer.fit_transform(df[\"transformed text\"]).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = tt(x,df[\"target\"],test_size=0.3,random_state=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.fit(x_train,y_train)\n",
    "x_pred = classifier.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score , precision_score\n",
    "accuracy_score(x_pred,y_test),precision_score(x_pred,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./src/vectorizer.pkl\",\"wb\") as f:\n",
    "    pickle.dump(vectorizer,f)\n",
    "\n",
    "with open(\"./src/classifier.pkl\",\"wb\") as f:\n",
    "    pickle.dump(classifier,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def textPreprocessor(text):\n",
    "    import nltk\n",
    "    from nltk.corpus import stopwords\n",
    "    from nltk.stem.porter import PorterStemmer\n",
    "    import string\n",
    "\n",
    "    stemmer = PorterStemmer()\n",
    "    # convert to lower case\n",
    "    text = text.lower()\n",
    "    # tokenize the text to individual words\n",
    "    text = nltk.word_tokenize(text)\n",
    "    # remove special characters and convert\n",
    "    lst = []\n",
    "    for word in text:\n",
    "        # stopwords : which helps in formation of sentences and has no special meaning\n",
    "        if word.isalnum() and word not in stopwords.words(\"english\") and word not in string.punctuation:\n",
    "            lst.append(stemmer.stem(word))\n",
    "\n",
    "    text = \" \".join(lst)\n",
    "\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./src/textPreprocessor.pkl\",\"wb\") as f:\n",
    "    pickle.dump(textPreprocessor,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! streamlit run app.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "projects",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
